var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = WidebandDoA","category":"page"},{"location":"#WidebandDoA","page":"Home","title":"WidebandDoA","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for WidebandDoA.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [WidebandDoA]","category":"page"},{"location":"#WidebandDoA.UniformNormalLocalProposal","page":"Home","title":"WidebandDoA.UniformNormalLocalProposal","text":"UniformNormalLocalProposal(mu, sigma)\n\nUniform proposal over the DoAs and a log-normal propsal over the SNR parameters of a source.\n\nArguments\n\nmu: Mean of the log-normal proposal on the SNR parameter.\nsigma: Standard deviation of the log-normal proposal on the SNR parameter.\n\nThis corresponds to the following proposal:\n\nbeginaligned\n    phi_j sim qleft(phiright) = mathsfUniformleft(-fracpi2 fracpi2right) \n    gamma_j sim qleft(gammaright) = textsfLog-Normalleft(textttmu textttsigmaright)\nendaligned\n\n\n\n\n\n","category":"type"},{"location":"#WidebandDoA.WidebandConditioned","page":"Home","title":"WidebandDoA.WidebandConditioned","text":"WidebandConditioned(model, y)\n\nmodel conditioned on y.\n\nArguments\n\nmodel::AbstractWidebandModel: Signal model.\ny::AbstractMatrix: Received data, where the rows are the channels (sersors), while the columns are the signals.\n\n\n\n\n\n","category":"type"},{"location":"#WidebandDoA.WidebandData","page":"Home","title":"WidebandDoA.WidebandData","text":"WidebandData(y, y_fft, y_power)\n\nReceived signal with pre-processing.\n\nArguments\n\ny: Received signal, where the rows are the channels (sersors), while the columns are the signals.\ny_fft: Received signal after applying a channel-wise FFT.\ny_power: Power of the received signal.\n\n\n\n\n\n","category":"type"},{"location":"#WidebandDoA.WidebandIsoIsoLikelihood","page":"Home","title":"WidebandDoA.WidebandIsoIsoLikelihood","text":"WidebandIsoIsoLikelihood(n_samples, n_fft, delay_filter, Δx, c, fs)\n\nCollapsed likelihood for a isotropic normal source prior and an isotropic normal noise prior.\n\nArguments\n\nn_samples::Int: Number of samples in received signal.\nn_fft::Int: Length of the latent source signal.\ndelay_filter::AbstractDelayFilter: Delay filter.\nΔx::AbstractVector: Inter-sensor delay in seconds.\nc::Real: Propagation speed of the medium in m/s.\nfs::Real: Sampling rate in Hz.\n\nGiven a parameter NamedTuple(phi[j], loglambda[j]), this likelihood computes:\n\nbeginaligned\n    log pleft(y mid phi_1k gamma_1k alpha beta right) \n    = -fracN + beta2 logleft(fracalpha2 + y^dagger (H Lambda H^dagger + mathrmI)^-1 y right)\n       - frac12 det left(H^dagger Lambda H + mathrmIright)  \n    = -fracN + beta2 logleft(fracalpha2\n       + y^dagger y - y^dagger H left( Lambda^-1 + H^dagger H right)^-1 H^dagger y right)\n       - frac12 detleft(Lambdaright) det left(Lambda^-1 + H^dagger H right) \nendaligned\n\nwhere\n\nLambda = mathrmdiagleft(\n    expleft( texttextsfloglambda0 right)\n    ldots\n    expleft( texttextsfloglambdak right)\n right)\n\n(Note that gamma_j in the paper is lambda[j] in the code, which is a bit confusing.)\n\n\n\n\n\n","category":"type"},{"location":"#WidebandDoA.WidebandIsoIsoModel","page":"Home","title":"WidebandDoA.WidebandIsoIsoModel","text":"WidebandIsoIsoModel(n_samples, Δx, c, fs, source_prior, α, β, order_prior, n_fft)\n\nModel for wideband signal model with isotropic normal source prior and isotropic normal noise prior.\n\nArguments\n\nn_samples::Int: Number of samples of the received signal.\nΔx::AbstractVector: Inter-sensor delay of the array in seconds.\nc::Real: Propagation speed of the medium in m/s.\nfs::Real: Sampling frequency in Hz.\nsource_prior::UnivariateDistribution: Prior on the SNR parameter (gamma_j in the paper) of the sources.\nα::Real: alpha hyperparameter of the inerse-gamma prior on the signal standard deviation (sigma in the paper; default: 0)\nβ::Real beta hyperparameter of the inerse-gamma prior on the signal standard deviation (sigma in the paper; default: 0)\norder_prior::DiscreteDistribution: Prior on the model order (k in the paper; default: NegativeBinomial(1/2 + 0.1, 0.1/(0.1 + 1)))\nn_fft::Int: Length of the source signals. (default: n_samples*2)\n\n\n\n\n\n","category":"type"},{"location":"#WidebandDoA.WidebandIsoIsoParam","page":"Home","title":"WidebandDoA.WidebandIsoIsoParam","text":"WidebandIsoIsoParam(phi, loglambda)\n\nLocal parameter of WidebandIsoIsoModel.\n\n\n\n\n\n","category":"type"},{"location":"#WidebandDoA.WidebandIsoSourcePrior","page":"Home","title":"WidebandDoA.WidebandIsoSourcePrior","text":"WidebandIsoSourcePrior(n_samples, n_fft, alpha, beta, order_prior, source_prior)\n\nPrior for wideband signal model, where a Gaussian prior with an isotropic covariance structure is assigned on the latent source signals.\n\nArguments\n\nsamples::Int: Number of samples in the received signal.\nn_fft::Int: Length of the latent signal (N^prime in the paper).\nalpha::Real: alpha hyperparameter for the inverse-gamma prior on the signal variance.\nbeta::Real: beta hyperparameter for the inverse-gamma prior on the signal variance.\norder_prior::DiscreteDistribution: Prior on the number of sources. (Prior on k in the paper.)\nsource_prior::UnivariateDistribution: Hyperprior on the SNR hyperparameter of the sources. (Prior on gamma in the paper.)\n\n\n\n\n\n","category":"type"},{"location":"#WidebandDoA.WindowedSinc","page":"Home","title":"WidebandDoA.WindowedSinc","text":"WindowedSinc(n_fft) <: AbstractDelayFilter\n\nClosed-form fractional delay filter by Pei and Lai[PL2012][PL2014]\n\nArguments\n\nn_fft::Int: Number of taps of the filter.\n\n[PL2012]: S. -C. Pei and Y. -C. Lai, \"Closed Form Variable Fractional Time Delay Using FFT,\" IEEE Signal Processing Letters, 2012.\n\n[PL2014]: S. -C. Pei and Y. -C. Lai, \"Closed form variable fractional delay using FFT with transition band trade-off,\" In Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS), 2014.\n\n\n\n\n\n","category":"type"},{"location":"#Base.rand-Tuple{Random.AbstractRNG, WidebandDoA.AbstractWidebandModel}","page":"Home","title":"Base.rand","text":"rand(rng, model)\n\nSample from a wideband signal model.\n\nArguments\n\nrng::Random.AbstractRNG\nmodel: Wideband signal model.\n\nReturns\n\nparams: NamedTuple containing the model parameters.\ndata: Simulated received data.\n\n\n\n\n\n","category":"method"},{"location":"#Base.rand-Tuple{Random.AbstractRNG, WidebandIsoIsoLikelihood, AbstractMatrix, AbstractVector}","page":"Home","title":"Base.rand","text":"rand(rng, likelihood::WidebandIsoIsoLikelihood, x, phi; prior, sigma)\n\nSample from the collapsed likelihood for the model with isotropic normal prior and isotropic normal noise.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nlikelihood::WidebandIsoIsoLikelihood: Likelihood.\nx::AbstractMatrix: Latent source signals, where rows are the signals and columns are sources.\nphi::AbstractVector: Direction-of-arrivals. \n\nKeyword Arguments\n\nprior: Prior object used to sample sigma if needed (default: nothing).\nsigma: Signal standard deviation. (Default samples from InverseGamma(prior.alpha, prior.beta))\n\nReturns\n\ny: A simulated received signal, where the rows are the channels (sensors) and the columns are received signals.\n\nThe sampling process is as follows:\n\nbeginaligned\n    epsilon sim mathcalN(0 sigma^2 mathrmI) \n    x         sim mathcalN(0 sigma^2 H Lambda H^top) \n    y         = x + epsilon  \nendaligned\n\nand the noise epsilon,\n\nbeginaligned\n    y         sim mathcalN(0 sigma^2 left( H Lambda H^top + mathrmI right))\nendaligned\n\nSampling from this distribution is as simple as\n\nbeginaligned\n  y = sigma H Lambda^12 z_x + sigma z_epsilon\nendaligned\n\nwhere z_x and z_epsilon are independent standard Gaussian vectors.\n\n\n\n\n\n","category":"method"},{"location":"#Base.rand-Tuple{Random.AbstractRNG, WidebandIsoSourcePrior}","page":"Home","title":"Base.rand","text":"rand(rng, prior::WidebandIsoSourcePrior, )\n\nSample from prior with isotropic source covariance prior.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nprior::WidebandIsoSourcePrior: Prior.\n\nKeyword Arguments\n\nk: Model order. (Default samples from prior.model_order.)\nsigma: Signal standard deviation. (Default samples from InverseGamma(prior.alpha, prior.beta).)\nphi: DoA of each sources. (Length must match k; default samples from the uniform distribution over the interval -pi2  pi2.)\nlambda: SNR parameter for each source. (Length must match k; default samples from prior.source_prior.)\n\nThe sampling process is:\n\nbeginaligned\n    k sim textttpriormodel_order \n    sigma sim texttextsfInv-Gamma(textttprioralpha texttextttpriorbeta) \n    phi_j mid k sim texttextsfUniformleft(-fracpi2 fracpi2right) \n    lambda_j mid k sim textttpriorsource_prior \n    x_j mid k sigma lambda_j sim mathcalNleft(0 sigma lambda_j mathrmI right)\nendaligned\n\nReturns\n\nparams: Parameter of wideband signal model. (keys: k, phi, lambda, sigma, x)\n\n\n\n\n\n","category":"method"},{"location":"#WidebandDoA.array_delay","page":"Home","title":"WidebandDoA.array_delay","text":"array_delay(filter, Δn)\n\nReturns the fourier domain fractional delay filters as a matrix H in mathbbR^ N times M times K . The fractional delay filters are the ones in:\n\n\n\n\n\n","category":"function"},{"location":"#WidebandDoA.inter_sensor_delay-Tuple{AbstractVector, AbstractVector, Any}","page":"Home","title":"WidebandDoA.inter_sensor_delay","text":"inter_sensor_delay(ϕ, Δx, c)\n\nCompute the inter-sensor delay matrix D in mathbbR^M times K in seconds for a linear array, where M = length(isd) is the number of sensors on the array, and K = length(isd) is the number of targets. The matrix is computed as follows:\n\nD_mk = fracDelta xm  sin(phik)c\n\nArguments\n\nϕ::AbstractVector: Vector of DoAs in radian. \nΔx::AbstractVector: Vector of inter-sensor delay of the array.\nc: Propagation speed of the medium.\n\nReturns\n\ndelays: Matrix containing the delays in seconds. Each row correspond to sensor, and each column correspond to the source.\n\n\n\n\n\n","category":"method"},{"location":"#WidebandDoA.loglikelihood","page":"Home","title":"WidebandDoA.loglikelihood","text":"loglikelihood(likelihood, params)\n\nLog likelihood of likelihood evaluated on params.\n\n\n\n\n\n","category":"function"},{"location":"#WidebandDoA.logpriordensity","page":"Home","title":"WidebandDoA.logpriordensity","text":"logpriordensity(prior, params)\n\nLog density of prior evaluated on params.\n\n\n\n\n\n","category":"function"},{"location":"#WidebandDoA.reconstruct-Tuple{WidebandConditioned, Any}","page":"Home","title":"WidebandDoA.reconstruct","text":"reconstruct(cond::WidebandConditioned, params)\n\nConditional posterior of the latent source signals for reconstruction given conditioned model cond and params\n\nArguments\n\ncond::WidebandConditioned: Conditioned model.\nparams: Additional parameters we need to condition on.\n\nReturns\n\ncond_post: Conditional posterior for the latent source signals.\n\n\n\n\n\n","category":"method"},{"location":"#WidebandDoA.relabel-Union{Tuple{T}, Tuple{Random.AbstractRNG, AbstractVector{<:AbstractVector{T}}, Int64}} where T<:Real","page":"Home","title":"WidebandDoA.relabel","text":"relabel(rng, samples, n_mixture; n_iter, n_imh_iter, show_progress)\n\nRelabel the RJMCMC samples samples into n_mixture Gaussian mixtures according to the stochastic expectation maximization (SEM) procedure of Roodaki et al. 2014[RBF2014]. \n\nArguments\n\nrng::Random.AbstractRNG\nsamples::AbstractVector{<:AbstractVector{<:Real}}: Samples subject to relabeling.\nn_mixture::Int: Number of component in the Gaussian mixture. Roodaki et al. recommend setting this as the 80% or 90% percentile of model order posterior.\n\nKeyword Arguments\n\nn_iter::Int: Number of SEM iterations (default: 16).\nn_mh_iter::Int: Number of Metropolis-Hastings steps for sampling an a label assignment (default: 32).\nshow_progress::Bool: Whether to enable progresss line (default: true).\n\nReturns\n\nmixture::Distributions.MixtureModel: The Gaussian mixture model fit over samples.\nlabels::Vector{Vector{Int}}: Labels assigned to each element of each RJCMCM sample.\n\nThe length of each RJCMCMC sample in samples is the model order of that specific sample. Each element of an RJCMCM sample should be the variables that determine which label this element should be associated with.\n\n[RBF2014]: Roodaki, Alireza, Julien Bect, and Gilles Fleury. \"Relabeling and summarizing posterior distributions in signal decomposition problems when the number of components is unknown.\" IEEE Transactions on Signal Processing (2014).\n\n\n\n\n\n","category":"method"},{"location":"baseline/#Validation-of-Baseline","page":"Baseline","title":"Validation of Baseline","text":"","category":"section"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"To validate the implementation of our key baseline, generalized likelihood ratio testing (GLRT) as proposed by Chung et al. [CBMH2007]. In particular, we reproduce Fig. 4 of the paper, which are the results for k=3 wideband sources. We will try to follow the experimental setup in the paper as closely as possible.","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"[CBMH2007]: Chung, P. J., Bohme, J. F., Mecklenbrauker, C. F., & Hero, A. O. (2007). Detection of the number of signals using the Benjamini-Hochberg procedure. IEEE Transactions on Signal Processing, 55(6), 2497-2508.","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"using Accessors\nusing FFTW\nusing FillArrays\nusing LinearAlgebra\nusing MKL\nusing NLopt\nusing Optim, LineSearches\nusing Peaks\nusing Random\nusing SpecialFunctions\nusing StatsBase\nusing StatsFuns\nusing Tullio\n\ninclude(\"../../scripts/baselines/common.jl\")\ninclude(\"../../scripts/baselines/directml.jl\")\ninclude(\"../../scripts/baselines/likeratiotest.jl\")\ninclude(\"../../scripts/common.jl\")\nnothing","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"Here is the system setup.","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"n_fft  = 64 # Number of requency bins (J in the original paper)\nn_snap = 10 # Number of snapshots (K in the original paper) \n\nfs      = 2000         # Sampling frequency [Hz]\nN       = n_fft*n_snap # Number of samples\nM       = 15           # Number of sensors\nf0      = fs/3         # Maximum frequency  [Hz]\nc       = 1500         # Propagation speed of the medium [m/s]\nλ       = c/f0         # Minimum wavelength \nspacing = λ/2          # Inter-sensor spacing [m]\nΔx      = range(0, M*spacing; length=M) # Relative sensor position\nnothing","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"Unfortunately, the original paper does not specify the sampling frequency and target frequency they used.  Therefore, we had to pick some arbitrary number for fs and f0.","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"The k = 3 targets and their bandwidths are set as followS:","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"ϕ   = [-30, 20, 24] / 180*π  # Direction-of-Arrivals\nΔf  = f0 - (17/32*f0)        # Signal bandwidth\nnothing","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"We now generate the source signals by band-limiting white Gaussian noise. ","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"using DSP\nusing Random123\n\nseed = (0x97dcb950eaebcfba, 0x741d36b68bef6415)\nrng  = Random123.Philox4x(UInt64, seed, 8)\nRandom123.set_counter!(rng, 1)\n\nϵ   = randn(rng, N, length(ϕ))\nbpf = DSP.Filters.digitalfilter(\n    DSP.Filters.Bandpass(17/32*f0, f0, fs=fs), \n    DSP.Filters.Butterworth(8)\n)\nx   = mapslices(xi -> DSP.Filters.filt(bpf, xi), ϵ; dims=1)\nnothing","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"GLRT uses the following key parameters:","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"k_max   = 4    # Maximum number of targets\nq       = 0.1  # False discovery rate\nnothing","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"The simulation range is set as follows:","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"snrs     = -8:2:6\nn_trials = 30\nnothing","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"Let's run the simulation.","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"filter   = WidebandDoA.WindowedSinc(N)\npcorrect = map(snrs) do snr\n    mean(1:n_trials) do _\n        σ2   = 10^(-snr/10)\n        like = WidebandIsoIsoLikelihood(N, 4*N, filter, Δx, c, fs)\n        y    = rand(rng, like, x, ϕ; sigma=sqrt(σ2*Δf/fs))\n\n        config        = ArrayConfig(c, Δx)\n        R, Y, f_range = snapshot_covariance(y, n_fft, fs, n_snap)\n\n        # The original paper only states that they use J = 10 frequency bins.\n        # We pick 10 bins that sufficiently cover the target frequency f0.\n        idx_sel = 13:22\n        R_sel   = R[:,:,idx_sel]\n        f_sel   = f_range[idx_sel]\n        Y_sel   = Y[:,:,idx_sel]\n\n        k, _ = likeratiotest(\n            rng, Y_sel, R_sel, q, k_max, n_snap, f_sel, config; visualize=false\n        )\n        k == length(ϕ)\n    end\nend","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"using Plots\n\nPlots.plot(snrs, pcorrect, xlabel=\"SNR\", ylabel=\"Prob. of Detection\", ylims=[0, 1])\nsavefig(\"baseline.svg\")\nnothing","category":"page"},{"location":"baseline/","page":"Baseline","title":"Baseline","text":"(Image: )","category":"page"},{"location":"demonstration/#Demonstration","page":"Demonstration","title":"Demonstration","text":"","category":"section"},{"location":"demonstration/#Setup","page":"Demonstration","title":"Setup","text":"","category":"section"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"We demonstrate the use of the package.","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"First, let's setup the array geometry. We will consider a system with a uniform linear array (ULA) with M = 20 sensors and a spacing of 0.5 m, a sampling frequency of fs = 30000Hz, where the medium has a propagation speed of c = 1500 m/s:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"M  = 20\nΔx = range(0, M*0.5; length=M)\nc  = 1500.\nfs = 3000.\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"For the target sources, we will generate k = 4 targets, with varying bandwidths and varying SNRs:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"k       = 4\nϕ       = [ -60,  -15,  30,  45]/180*π # True direction-of-arrivals\nf_begin = [  10,  100,  50, 800]       # Source signal starting frequency\nf_end   = [1000, 1500, 500, 900]       # Source signal ending frequency\nsnr     = [  -6,   -4,   0,   4]       # Varying SNRs in dB\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"For simulating the signals, let's use a utility function we used for the experiments. The length of the simulated signal will be N = 128","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"using Random, Random123\n\nseed = (0x97dcb950eaebcfba, 0x741d36b68bef6415)\nrng  = Random123.Philox4x(UInt64, seed, 8)\nRandom123.set_counter!(rng, 1)\n\nN = 256\n\ninclude(\"../../scripts/common.jl\")\ny, x  = simulate_signal(\n    rng, N, N*8, ϕ, snr, f_begin, f_end, fs, 1.0, Δx, c; visualize=false\n)\ny","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"We can visualize the simulated signal by beamformer-based power estimators. For instance, applying the Capon spectral estimator, more commonly known as minimum-variance distortionless response, or MVDR for short:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"using Tullio\nusing Plots\n\ninclude(\"../../scripts/baselines/common.jl\")\ninclude(\"../../scripts/baselines/subbandbeamform.jl\")\n\nN_fft  = 32\nN_snap = N ÷ N_fft\n\nR, _, f_range = snapshot_covariance(y, N_fft, fs, N_snap) # Short-time Fourier transform\n\nconfig  = ArrayConfig(c, Δx)\nn_grid  = 2^10\nϕ_range = range(-π/2, π/2; length=n_grid)\nP       = subbandmvdr(R, ϕ_range, f_range, config)\nPlots.heatmap(ϕ_range, f_range, 10*log10.(P'), xlabel=\"DoA (ϕ)\", ylabel=\"Frequency\")\nPlots.vline!(ϕ, label=\"True DoAs\", linecolor=:red)\nsavefig(\"angle_frequency_spectrum_plot.svg\")\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Here is the angle-frequency plot:  (Image: )","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Although the high SNR signal at 45 degress is obscuring other signals, we can see that the signal were correctly simualted as we specified.","category":"page"},{"location":"demonstration/#Creating-the-Bayesian-Model","page":"Demonstration","title":"Creating the Bayesian Model","text":"","category":"section"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Now, let's create the Bayesian model. We will use a non-informating prior on the source SNRs (gamma in the paper), the truncated negative-binomial prior on the model order, a Jeffrey's scale prior on the signal power (alpha = beta = 0) as stated in the paper:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"alpha, beta  = 0, 0\nsource_prior = InverseGamma(0.01, 0.01)\norder_prior  = truncated(NegativeBinomial(1/2 + 0.1, 0.1/(0.1 + 1)), 0, M-1)\nmodel        = WidebandDoA.WidebandIsoIsoModel(\n    N, Δx, c, fs, source_prior, alpha, beta; order_prior\n)\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"The model can be conditioned on the data as follows:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"cond  = WidebandConditioned(model, y)\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"We are now ready to infer the posterior for this model.","category":"page"},{"location":"demonstration/#Inference-with-RJMCMC","page":"Demonstration","title":"Inference with RJMCMC","text":"","category":"section"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"For inference, we use ReversibleJump package, which is the inference counterpart of this package.","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"We will use independent jump proposals with the uniform-log-normal auxiliary proposal distributions (q(gamma), q(gamma) in the paper) stated in the paper:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"using ReversibleJump\n\nprop = UniformNormalLocalProposal(0.0, 2.0)\njump = IndepJumpProposal(prop)\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"For the update move, we will use slice sampling[N2003] with the stepping out procedure:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"mcmc = SliceSteppingOut([2.0, 2.0])\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"[N2003]: Neal, Radford M. \"Slice sampling.\" The annals of statistics 31.3 (2003): 705-767.","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"The RJMCMC algorithm we use is the non-reversible jump algorithm by Gagnon and Doucet[GD2020].","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"rjmcmc = ReversibleJump.NonReversibleJumpMCMC(jump, mcmc; jump_rate=0.9)\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"[GD2020]: Gagnon, Philippe, and Arnaud Doucet. \"Nonreversible jump algorithms for Bayesian nested model selection.\" Journal of Computational and Graphical Statistics 30.2 (2020): 312-323.","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Now let's simulate some Markov chains!","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"n_samples = 4_000\n\ninitial_params = WidebandDoA.WidebandIsoIsoParam{Float64}[]\ninitial_order  = 0\nsamples, stats = ReversibleJump.sample(\n    rng,\n    rjmcmc,\n    cond,\n    n_samples,\n    initial_order,\n    initial_params;\n    show_progress=false,\n)\nsamples","category":"page"},{"location":"demonstration/#Signal-Detection","page":"Demonstration","title":"Signal Detection","text":"","category":"section"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Let's inspect the Markov chain.","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"First, we will look at the posterior of the model order:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Plots.plot([stat.order for stat in stats],  xlabel=\"RJMCMC Iteration\",  ylabel=\"Model order\")\nsavefig(\"model_order_trace.svg\")\n\nPlots.histogram([stat.order for stat in stats], xlabel=\"order\", normed=true)\nsavefig(\"model_order_hist.svg\")\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Here is the trace of the model order:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"(Image: )","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Here is the histogram of the model order:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"(Image: )","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Point estimates for the model order can now be obtained from the posterior. In the paper, we use the median. But using the posterior mode yields similar performs except for a narrow performance region where the model transitions from now working very well to working very well. ","category":"page"},{"location":"demonstration/#DoA-Estimation","page":"Demonstration","title":"DoA Estimation","text":"","category":"section"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Now, let's look at the DoA estimates.","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"For this, we will discard the first 10% of the samples and only use the remaining samples.","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"burned = samples[n_samples ÷ 10:end]","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Bayesian model averaging (BMA) correponds to flattening all the local variables:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"flat = vcat(burned...)","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Instead, one can also select the samples that have a specific model order. This corresponds to conditioning on the model (order) we selected, which is Bayesian model selection.","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Here is the marginal posterior of the DoAs:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Plots.histogram([θ.phi for θ in flat], normed=true, bins=128, xlims=[-π/2, π/2], xlabel=\"DoA (ϕ)\")\nPlots.vline!(ϕ, label=\"True\", color=:red, linestyle=:dash)\nsavefig(\"doa_hist.svg\")\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"(Image: )","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Unfortunately, it is hard to construct a histogram of the DoA posterior samples for each source. For this, we turn to the relabeling algorithm by Roodaki et al.[RBF2014]. This algorithm fits a Gaussian mixture model on the histogram above. It also generates labels for each local variable, so that we can label variable other than just the DoAs. For this though, we have to choose the number of mixture components.  Roodaki et al. recommend the 80% or 90% upper percentile of the posterior:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"k_mixture = quantile([stat.order for stat in stats], 0.9) |> Base.Fix1(round, Int)\nϕ_post    = [[target.phi for target in sample] for sample in burned]\nmixture, labels = WidebandDoA.relabel(\n    rng, ϕ_post, k_mixture; show_progress=false\n)\nmixture","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"[RBF2014]: Roodaki, Alireza, Julien Bect, and Gilles Fleury. \"Relabeling and summarizing posterior distributions in signal decomposition problems when the number of components is unknown.\" IEEE Transactions on Signal Processing (2014).","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"We can compare the components with the marginal mixture:","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"using StatsPlots\n\nPlots.stephist([θ.phi for θ in flat], normed=true, bins=128, xlims=[-π/2, π/2], xlabel=\"DoA (ϕ)\", label=\"Posterior\", fill=true)\nPlots.plot!(mixture, label=\"Component\")\nPlots.vline!(ϕ, label=\"True\", color=:red, linestyle=:dash)\nsavefig(\"doa_relabel_hist.svg\")\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"(Image: )","category":"page"},{"location":"demonstration/#Reconstruction","page":"Demonstration","title":"Reconstruction","text":"","category":"section"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"We will finally demonstrate reconstruction. Unforutnately, the API for reconstruction is a little less ironed-out, but it is usable. In addition, we have to use the labels generated by the relabeling procedure and label the RJMCMC samples. Here, we will sample from the conditional posterior conditional on each RJMCMC sample and relabel the samples at the same time. We also thin the samples by a factor n_thin = 100 to speed up things and reduce memory consumption.","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"x_samples = [Vector{Float64}[] for j in 1:k_mixture]\nx_means   = [Vector{Float64}[] for j in 1:k_mixture]\n\nn_thin = n_samples ÷ 10\n\nsamples_thinned = burned[1:n_thin:end]\nlabels_thinned  = labels[1:n_thin:end]\n\nfor (sample, labs) in zip(samples_thinned, labels_thinned)\n    dist_x   = WidebandDoA.reconstruct(cond, sample)\n    x_sample = rand(rng, dist_x) # Conditional posterior sample\n    x_mean   = mean(dist_x) # Conditional posterior mean\n\n    kj        = length(sample)\n    total_len = length(x_sample)\n    blocksize = total_len ÷ kj\n\n    # Labeling the conditional posterior mean and sample\n    for (idx, label) in enumerate(labs)\n        if label > k_mixture\n            # A label of k_mixture + 1 corresponds to the clutter\n            continue\n        end\n\n        # The source signals are flattened so we have to slice the block corresponding\n        # to the source the label is pointing to.\n        blockrange = (idx-1)*blocksize+1:idx*blocksize\n        xj_sample  = x_sample[blockrange[1:N]]\n        xj_mean    = x_mean[  blockrange[1:N]]\n        push!(x_samples[label], xj_sample)\n        push!(x_means[label],   xj_mean)\n    end\nend\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Now that sampling and relabeling is done, let's visualize the posterior samples against the MMSE estimates (posterior mean)","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"x_mmse = mean.(x_samples)\n\nplts = map(1:k_mixture) do j\n    p = Plots.plot(x_mmse[j], linecolor=:blue, label=\"MMSE\", xlabel=\"Sample index\")\n    for x_sample in x_samples[j]\n        Plots.plot!(p, x_sample, linecolor=:blue, alpha=0.5, linewidth=0.2, label=nothing)\n    end\n    p\nend\nPlots.plot(plts..., layout = (4, 1))\nsavefig(\"recon_samples.svg\")\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"(Image: )","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Finally, let's compare the results against the ground truth x.","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"Plots.plot( x_mmse, layout=(4,1), label=\"MMSE\", xlabel=\"Sample Index\")\nPlots.plot!(x,      layout=(4,1), label=\"True\", xlabel=\"Sample Index\")\nsavefig(\"recon_mmse_comparison.svg\")\nnothing","category":"page"},{"location":"demonstration/","page":"Demonstration","title":"Demonstration","text":"(Image: )","category":"page"}]
}
